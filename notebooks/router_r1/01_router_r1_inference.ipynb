{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RouterR1 - Inference\n",
    "\n",
    "This notebook demonstrates how to use **RouterR1** for agentic routing and reasoning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "RouterR1 is an agentic router that performs R1-like reasoning with iterative search and routing.\n",
    "It uses vLLM for local inference and an external routing API for model selection.\n",
    "\n",
    "**Key Features**:\n",
    "- Agentic reasoning with iterative search\n",
    "- Pre-trained model (no training required)\n",
    "- Uses vLLM for efficient GPU inference\n",
    "- Integrates with external routing pool API\n",
    "\n",
    "**Requirements**:\n",
    "- CUDA-enabled GPU\n",
    "- vLLM library\n",
    "- OpenAI-compatible API endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Ensure GPU runtime is enabled\n",
    "# Runtime -> Change runtime type -> Hardware accelerator -> GPU (T4 or better)\n",
    "\n",
    "import os\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !git clone https://github.com/ulab-uiuc/LLMRouter.git\n",
    "    %cd LLMRouter\n",
    "    !pip install -e .\n",
    "    !pip install vllm transformers pyyaml openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability (required for RouterR1)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"WARNING: RouterR1 requires CUDA. Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.utils import setup_environment\n",
    "import yaml\n",
    "\n",
    "setup_environment()\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "RouterR1 requires the following configuration parameters:\n",
    "\n",
    "| Parameter | Description | Required |\n",
    "|-----------|-------------|----------|\n",
    "| `model_id` | HuggingFace model ID for reasoning | Yes |\n",
    "| `api_base` | Routing API endpoint URL | Yes |\n",
    "| `api_key` | API key for routing service | Yes |\n",
    "\n",
    "**Note**: Create a config file or set these parameters programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for RouterR1\n",
    "# Replace with your actual API credentials\n",
    "\n",
    "router_r1_config = {\n",
    "    \"data_path\": {\n",
    "        \"query_data_train\": \"data/example_data/query_data/default_query_train.jsonl\",\n",
    "        \"query_data_test\": \"data/example_data/query_data/default_query_test.jsonl\",\n",
    "        \"routing_data_train\": \"data/example_data/routing_data/default_routing_train_data.jsonl\",\n",
    "        \"routing_data_test\": \"data/example_data/routing_data/default_routing_test_data.jsonl\",\n",
    "        \"llm_data\": \"data/example_data/llm_candidates/default_llm.json\"\n",
    "    },\n",
    "    \"hparam\": {\n",
    "        \"model_id\": \"Qwen/Qwen2.5-3B-Instruct\",  # Pre-trained model for reasoning\n",
    "        \"api_base\": \"https://api.openai.com/v1\",  # Replace with your API endpoint\n",
    "        \"api_key\": os.environ.get(\"OPENAI_API_KEY\", \"your-api-key-here\")  # Replace with your API key\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config to temporary file\n",
    "CONFIG_PATH = \"configs/model_config_train/router_r1_temp.yaml\"\n",
    "os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n",
    "\n",
    "with open(CONFIG_PATH, 'w') as f:\n",
    "    yaml.dump(router_r1_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(yaml.dump(router_r1_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.models.router_r1 import RouterR1\n",
    "\n",
    "# Initialize RouterR1 (requires valid API credentials)\n",
    "try:\n",
    "    router = RouterR1(yaml_path=CONFIG_PATH)\n",
    "    print(\"RouterR1 initialized successfully!\")\n",
    "    print(f\"Model ID: {router.model_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing RouterR1: {e}\")\n",
    "    print(\"Please ensure you have valid API credentials configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Query Routing\n",
    "\n",
    "RouterR1 performs agentic reasoning with iterative search and routing.\n",
    "The process includes:\n",
    "1. Initial prompt generation\n",
    "2. Iterative reasoning with search queries\n",
    "3. External routing API calls\n",
    "4. Final answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query for RouterR1\n",
    "test_query = {\"query\": \"What is the capital of France and what is its population?\"}\n",
    "\n",
    "print(f\"Query: {test_query['query']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Route with details (includes token counts)\n",
    "try:\n",
    "    result = router.route_single(test_query, return_details=True)\n",
    "    \n",
    "    print(\"\\nResponse:\")\n",
    "    print(result[\"response\"])\n",
    "    print(\"\\nToken Usage:\")\n",
    "    print(f\"  Prompt tokens: {result['prompt_tokens']}\")\n",
    "    print(f\"  Completion tokens: {result['completion_tokens']}\")\n",
    "    print(f\"  Route tokens: {result['route_tokens']}\")\n",
    "    print(f\"  Total tokens: {result['total_tokens']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during routing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch routing with multiple queries\n",
    "batch_queries = [\n",
    "    {\"query\": \"What is machine learning?\"},\n",
    "    {\"query\": \"Explain quantum computing in simple terms.\"},\n",
    "    {\"query\": \"How does photosynthesis work?\"},\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(batch_queries)} queries...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    results = router.route_batch(batch_queries)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Query: {result.get('query', 'N/A')[:50]}...\")\n",
    "        print(f\"   Success: {result.get('success', False)}\")\n",
    "        print(f\"   Response length: {len(result.get('response', ''))} chars\")\n",
    "        print(f\"   Tokens: {result.get('prompt_tokens', 0) + result.get('completion_tokens', 0)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during batch routing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task-Specific Routing\n",
    "\n",
    "RouterR1 can format queries for specific tasks (MMLU, GSM8K, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-specific routing with ground truth for evaluation\n",
    "task_query = {\n",
    "    \"query\": \"What is the derivative of x^2?\",\n",
    "    \"task_name\": \"math\",\n",
    "    \"ground_truth\": \"2x\"\n",
    "}\n",
    "\n",
    "print(f\"Task Query: {task_query['query']}\")\n",
    "print(f\"Task: {task_query['task_name']}\")\n",
    "print(f\"Ground Truth: {task_query['ground_truth']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    results = router.route_batch([task_query], task_name=\"math\")\n",
    "    result = results[0]\n",
    "    \n",
    "    print(f\"\\nResponse: {result.get('response', 'N/A')[:200]}...\")\n",
    "    if 'task_performance' in result:\n",
    "        print(f\"Task Performance: {result['task_performance']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. File-Based Inference\n\nLoad queries from a file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from default query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/router_r1_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries (limit to 5 for demo due to API costs)\n    try:\n        file_results = router.route_batch(file_queries[:5])\n        print(f\"Routed {len(file_results)} queries\")\n        \n        # Save results\n        save_results_to_file(file_results, OUTPUT_FILE)\n        \n        # Show sample results\n        print(f\"\\nSample results:\")\n        for i, result in enumerate(file_results[:3], 1):\n            print(f\"  {i}. {result.get('query', '')[:40]}...\")\n            print(f\"     Success: {result.get('success', False)}\")\n    except Exception as e:\n        print(f\"Error during batch routing: {e}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**RouterR1 Key Points**:\n",
    "- Pre-trained agentic router (no training required)\n",
    "- Uses vLLM for efficient GPU inference\n",
    "- Iterative reasoning with external routing API\n",
    "- Supports task-specific query formatting\n",
    "- Token tracking for cost analysis\n",
    "\n",
    "**Requirements**:\n",
    "- CUDA-enabled GPU (vLLM requirement)\n",
    "- Valid API credentials for routing service\n",
    "- Sufficient GPU memory for the base model\n",
    "\n",
    "**Use Cases**:\n",
    "- Complex queries requiring reasoning\n",
    "- Multi-step question answering\n",
    "- Research and evaluation on routing strategies"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}