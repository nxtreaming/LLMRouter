{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMTRouter - Inference\n",
    "\n",
    "This notebook demonstrates how to use a trained **GMTRouter** for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llmrouter.models.gmtrouter import GMTRouter\n",
    "from llmrouter.utils import setup_environment\n",
    "import yaml\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/model_config_train/gmtrouter.yaml\"\n",
    "\n",
    "router = GMTRouter(yaml_path=CONFIG_PATH)\n",
    "print(\"Router loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Personalized Query Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation example\n",
    "CONVERSATION = [\n",
    "    {\n",
    "        \"query\": \"What is machine learning?\",\n",
    "        \"user_id\": \"user_001\",\n",
    "        \"session_id\": \"session_001\",\n",
    "        \"turn\": 1\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can you give me a practical example?\",\n",
    "        \"user_id\": \"user_001\",\n",
    "        \"session_id\": \"session_001\",\n",
    "        \"turn\": 2\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I implement this in Python?\",\n",
    "        \"user_id\": \"user_001\",\n",
    "        \"session_id\": \"session_001\",\n",
    "        \"turn\": 3\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Multi-turn Routing Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in CONVERSATION:\n",
    "    result = router.route_single(query)\n",
    "    print(f\"Turn {query['turn']}: {query['query'][:40]}...\")\n",
    "    print(f\"   Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Different User Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same query, different users\n",
    "query_text = \"Explain neural networks.\"\n",
    "\n",
    "users = [\"user_001\", \"user_002\", \"user_003\"]\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for user in users:\n",
    "    query = {\n",
    "        \"query\": query_text,\n",
    "        \"user_id\": user,\n",
    "        \"session_id\": f\"{user}_session\"\n",
    "    }\n",
    "    result = router.route_single(query)\n",
    "    print(f\"{user}: Routed to {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5. File-Based Inference\n\nLoad queries from a file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from default query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/gmtrouter_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries\n    file_results = router.route_batch(batch=file_queries[:10])\n    print(f\"Routed {len(file_results)} queries\")\n    \n    # Save results\n    save_results_to_file(file_results, OUTPUT_FILE)\n    \n    # Show sample results\n    print(f\"\\nSample results:\")\n    for i, result in enumerate(file_results[:3], 1):\n        print(f\"  {i}. {result.get('query', '')[:40]}... -> {result['model_name']}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "GMTRouter provides:\n",
    "- Personalized routing based on user history\n",
    "- Multi-turn context awareness\n",
    "- Graph-based relationship modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}