{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNRouter - Inference\n",
    "\n",
    "This notebook demonstrates how to use a trained **KNNRouter** for inference.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will cover:\n",
    "1. Loading a trained KNNRouter model\n",
    "2. Single query routing\n",
    "3. Batch query routing\n",
    "4. Full inference with API calls\n",
    "5. Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "# !pip install llmrouter scikit-learn transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from llmrouter.models.knnrouter import KNNRouter\n",
    "from llmrouter.utils import setup_environment, load_model, get_longformer_embedding\n",
    "\n",
    "setup_environment()\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Use inference configuration\n",
    "# The inference config should have load_model_path set\n",
    "CONFIG_PATH = \"configs/model_config_train/knnrouter.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Add load_model_path for inference\n",
    "config['model_path']['load_model_path'] = config['model_path'].get(\n",
    "    'load_model_path', \n",
    "    config['model_path']['save_model_path']\n",
    ")\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Model path: {config['model_path']['load_model_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference config file\n",
    "INFERENCE_CONFIG_PATH = \"configs/model_config_test/knnrouter_inference.yaml\"\n",
    "\n",
    "os.makedirs(os.path.dirname(INFERENCE_CONFIG_PATH), exist_ok=True)\n",
    "\n",
    "inference_config = config.copy()\n",
    "inference_config['model_path']['load_model_path'] = 'saved_models/knnrouter/knnrouter.pkl'\n",
    "\n",
    "with open(INFERENCE_CONFIG_PATH, 'w') as f:\n",
    "    yaml.dump(inference_config, f)\n",
    "\n",
    "print(f\"Inference config saved to: {INFERENCE_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize router for inference\n",
    "router = KNNRouter(yaml_path=INFERENCE_CONFIG_PATH)\n",
    "\n",
    "print(\"Router loaded successfully!\")\n",
    "print(f\"Number of LLM candidates: {len(router.llm_data)}\")\n",
    "print(f\"LLM candidates: {list(router.llm_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained KNN model\n",
    "model_path = os.path.join(PROJECT_ROOT, inference_config['model_path']['load_model_path'])\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    knn_model = load_model(model_path)\n",
    "    print(f\"Loaded model from: {model_path}\")\n",
    "    print(f\"Model classes: {knn_model.classes_}\")\n",
    "else:\n",
    "    print(f\"Model not found at: {model_path}\")\n",
    "    print(\"Please run the training notebook first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Query Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries for different task types\n",
    "EXAMPLE_QUERIES = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"task_type\": \"world_knowledge\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Solve the equation: 2x + 5 = 15\",\n",
    "        \"task_type\": \"math\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Write a Python function to check if a number is prime.\",\n",
    "        \"task_type\": \"code\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?\",\n",
    "        \"task_type\": \"reasoning\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the theory of relativity in simple terms.\",\n",
    "        \"task_type\": \"explanation\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(EXAMPLE_QUERIES)} example queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route a single query\n",
    "def route_single_query(query_dict):\n",
    "    \"\"\"Route a single query and return the result.\"\"\"\n",
    "    result = router.route_single(query_dict)\n",
    "    return result\n",
    "\n",
    "# Test with first example\n",
    "query = EXAMPLE_QUERIES[0]\n",
    "result = route_single_query(query)\n",
    "\n",
    "print(f\"Query: {query['query']}\")\n",
    "print(f\"Task Type: {query['task_type']}\")\n",
    "print(f\"Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route all example queries\n",
    "print(\"Routing Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(EXAMPLE_QUERIES, 1):\n",
    "    result = route_single_query(query)\n",
    "    print(f\"\\n{i}. Query: {query['query'][:60]}...\")\n",
    "    print(f\"   Task: {query['task_type']}\")\n",
    "    print(f\"   Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get Routing Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_routing_probabilities(query_text):\n",
    "    \"\"\"Get routing probabilities for all LLM candidates.\"\"\"\n",
    "    # Generate embedding\n",
    "    embedding = get_longformer_embedding(query_text).numpy().reshape(1, -1)\n",
    "    \n",
    "    # Get probabilities\n",
    "    proba = knn_model.predict_proba(embedding)[0]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = dict(zip(knn_model.classes_, proba))\n",
    "    \n",
    "    # Sort by probability\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with first query\n",
    "query_text = EXAMPLE_QUERIES[0]['query']\n",
    "probabilities = get_routing_probabilities(query_text)\n",
    "\n",
    "print(f\"Query: {query_text}\")\n",
    "print(\"\\nRouting Probabilities:\")\n",
    "for model, prob in probabilities.items():\n",
    "    bar = \"#\" * int(prob * 50)\n",
    "    print(f\"  {model:30} {prob:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize routing probabilities for all queries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(EXAMPLE_QUERIES), 1, figsize=(12, 3*len(EXAMPLE_QUERIES)))\n",
    "\n",
    "for idx, query in enumerate(EXAMPLE_QUERIES):\n",
    "    probs = get_routing_probabilities(query['query'])\n",
    "    \n",
    "    ax = axes[idx] if len(EXAMPLE_QUERIES) > 1 else axes\n",
    "    models = list(probs.keys())\n",
    "    values = list(probs.values())\n",
    "    \n",
    "    bars = ax.barh(models, values, color='steelblue')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_title(f\"Query {idx+1}: {query['query'][:50]}... ({query['task_type']})\")\n",
    "    \n",
    "    # Highlight the chosen model\n",
    "    max_idx = values.index(max(values))\n",
    "    bars[max_idx].set_color('green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Batch Query Routing\n\n### Option 1: Route queries from configuration file\n\nThe router automatically loads test data from the path specified in `query_data_test` in the YAML config."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for batch routing\n",
    "if router.query_data_test is not None:\n",
    "    test_data = router.query_data_test[:20]  # Use first 20 samples\n",
    "    print(f\"Loaded {len(test_data)} test samples\")\n",
    "else:\n",
    "    print(\"No test data available. Using example queries.\")\n",
    "    test_data = EXAMPLE_QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch routing (route-only, no API calls)\n",
    "def batch_route_only(queries):\n",
    "    \"\"\"Route multiple queries without calling APIs.\"\"\"\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        result = router.route_single(query)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Route batch\n",
    "batch_results = batch_route_only(test_data)\n",
    "\n",
    "print(f\"Routed {len(batch_results)} queries\")\n",
    "\n",
    "# Show routing distribution\n",
    "from collections import Counter\n",
    "model_counts = Counter(r['model_name'] for r in batch_results)\n",
    "\n",
    "print(\"\\nRouting Distribution:\")\n",
    "for model, count in model_counts.most_common():\n",
    "    percentage = count / len(batch_results) * 100\n",
    "    print(f\"  {model}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Option 2: Load queries from your own file\n\nYou can also load queries from a custom JSONL file and pass them to the router.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Method 1: Load from custom JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Example: Load from the default query test file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from file\")\n    print(f\"\\nSample query: {file_queries[0]}\")\n    \n    # Route queries from file\n    file_results = router.route_batch(batch=file_queries[:10])\n    \n    print(f\"\\nRouted {len(file_results)} queries from file:\")\n    for i, result in enumerate(file_results[:3], 1):\n        print(f\"  {i}. Query: {result.get('query', '')[:50]}...\")\n        print(f\"     Routed to: {result['model_name']}\")\nelse:\n    print(f\"File not found: {QUERY_FILE}\")\n    print(\"You can create your own JSONL file with format:\")\n    print('  {\"query\": \"Your question here\"}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 2: Save routing results to file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Save results\nOUTPUT_FILE = \"outputs/knnrouter_results.jsonl\"\nos.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n\nif 'file_results' in dir() and file_results:\n    save_results_to_file(file_results, OUTPUT_FILE)\n    \n    # Verify saved file\n    print(f\"\\nVerifying saved file:\")\n    with open(OUTPUT_FILE, 'r') as f:\n        first_line = json.loads(f.readline())\n        print(f\"First result: {first_line}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize routing distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = list(model_counts.keys())\n",
    "counts = list(model_counts.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(counts, labels=models, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Query Routing Distribution')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Inference with API Calls (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API keys are available\n",
    "api_available = bool(\n",
    "    os.environ.get('OPENAI_API_KEY') or \n",
    "    os.environ.get('ANTHROPIC_API_KEY') or\n",
    "    os.environ.get('API_KEYS')\n",
    ")\n",
    "\n",
    "print(f\"API keys available: {api_available}\")\n",
    "\n",
    "if not api_available:\n",
    "    print(\"\\nTo enable full inference with API calls, set one of:\")\n",
    "    print(\"  - OPENAI_API_KEY\")\n",
    "    print(\"  - ANTHROPIC_API_KEY\")\n",
    "    print(\"  - API_KEYS (JSON array of keys)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full inference (with API calls) - only if API keys are available\n",
    "if api_available:\n",
    "    # Use route_batch which includes API calls\n",
    "    full_results = router.route_batch(batch=test_data[:5])  # Limit to 5 for demo\n",
    "    \n",
    "    print(\"Full Inference Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result in full_results:\n",
    "        print(f\"\\nQuery: {result['query'][:60]}...\")\n",
    "        print(f\"Routed to: {result['model_name']}\")\n",
    "        print(f\"Response: {result.get('response', 'N/A')[:100]}...\")\n",
    "        print(f\"Success: {result.get('success', 'N/A')}\")\n",
    "else:\n",
    "    print(\"Skipping full inference - no API keys configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate routing accuracy on test data\n",
    "# This compares the router's choice with the oracle (best performing model)\n",
    "\n",
    "if router.routing_data_test is not None:\n",
    "    test_df = router.routing_data_test\n",
    "    \n",
    "    # Find best model for each query (oracle)\n",
    "    oracle_best = test_df.loc[\n",
    "        test_df.groupby('query')['performance'].idxmax()\n",
    "    ][['query', 'model_name', 'performance']]\n",
    "    oracle_best.columns = ['query', 'oracle_model', 'oracle_performance']\n",
    "    \n",
    "    print(f\"Test set: {len(oracle_best)} unique queries\")\n",
    "    print(f\"\\nOracle model distribution:\")\n",
    "    print(oracle_best['oracle_model'].value_counts())\n",
    "else:\n",
    "    print(\"No test routing data available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare router predictions with oracle\n",
    "if router.routing_data_test is not None:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_comparison = []\n",
    "    \n",
    "    for _, row in tqdm(oracle_best.iterrows(), total=len(oracle_best), desc=\"Evaluating\"):\n",
    "        query = row['query']\n",
    "        oracle_model = row['oracle_model']\n",
    "        \n",
    "        # Get router prediction\n",
    "        result = router.route_single({'query': query})\n",
    "        predicted_model = result['model_name']\n",
    "        \n",
    "        is_correct = predicted_model == oracle_model\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "        \n",
    "        results_comparison.append({\n",
    "            'query': query[:50],\n",
    "            'oracle': oracle_model,\n",
    "            'predicted': predicted_model,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nRouting Accuracy: {accuracy:.4f} ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "if router.routing_data_test is not None:\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    comparison_df = pd.DataFrame(results_comparison)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(comparison_df['oracle'], comparison_df['predicted']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    labels = sorted(set(comparison_df['oracle']) | set(comparison_df['predicted']))\n",
    "    cm = confusion_matrix(comparison_df['oracle'], comparison_df['predicted'], labels=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Oracle')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using CLI for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the CLI for inference\n",
    "print(\"CLI Commands for Inference:\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"# Route a single query (route-only, no API call):\")\n",
    "print('llmrouter infer --router knnrouter --config configs/model_config_test/knnrouter_inference.yaml --query \"What is AI?\" --route-only')\n",
    "print()\n",
    "print(\"# Route with full inference (API call):\")\n",
    "print('llmrouter infer --router knnrouter --config configs/model_config_test/knnrouter_inference.yaml --query \"What is AI?\"')\n",
    "print()\n",
    "print(\"# Batch inference from file:\")\n",
    "print('llmrouter infer --router knnrouter --config configs/model_config_test/knnrouter_inference.yaml --input queries.txt --output results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded Trained Model**: Set up KNNRouter with trained model\n",
    "2. **Single Query Routing**: Routed individual queries to LLMs\n",
    "3. **Routing Probabilities**: Analyzed routing confidence\n",
    "4. **Batch Routing**: Processed multiple queries efficiently\n",
    "5. **Full Inference**: Called LLM APIs (when available)\n",
    "6. **Performance Evaluation**: Compared with oracle performance\n",
    "\n",
    "**Key Findings**:\n",
    "- KNNRouter provides interpretable routing decisions\n",
    "- Routing probabilities show model confidence\n",
    "- Performance can be tuned via K and distance metric\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different routers (SVMRouter, MLPRouter, etc.)\n",
    "- Experiment with ensemble routing\n",
    "- Deploy as API service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}