{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LargestLLM Router - Inference\n",
    "\n",
    "This notebook demonstrates the **LargestLLM** baseline router.\n",
    "\n",
    "## Overview\n",
    "\n",
    "LargestLLM is a simple baseline router that always routes queries to the largest model in the candidate pool.\n",
    "This serves as an upper bound for routing performance and a lower bound for cost efficiency.\n",
    "\n",
    "**Key Characteristics**:\n",
    "- No training required (deterministic baseline)\n",
    "- Always selects the largest model by parameter size\n",
    "- Useful for performance benchmarking\n",
    "- Highest expected quality, highest cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Clone repository and install dependencies\n",
    "import os\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !git clone https://github.com/ulab-uiuc/LLMRouter.git\n",
    "    %cd LLMRouter\n",
    "    !pip install -e .\n",
    "    !pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.models.largest_llm import LargestLLMRouter\n",
    "from llmrouter.utils import setup_environment\n",
    "import yaml\n",
    "\n",
    "setup_environment()\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "LargestLLM router requires only data paths - no hyperparameters.\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `llm_data` | Path to LLM candidate metadata |\n",
    "| `routing_data_test` | Path to test routing data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/model_config_train/largest_llm.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = LargestLLMRouter(yaml_path=CONFIG_PATH)\n",
    "\n",
    "print(\"Router initialized successfully!\")\n",
    "print(f\"Number of LLM candidates: {len(router.llm_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available LLM candidates sorted by size\n",
    "print(\"Available LLM Candidates (by size):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "llm_list = [(name, info.get('size', 'unknown')) for name, info in router.llm_data.items()]\n",
    "llm_list_sorted = sorted(llm_list, key=lambda x: float(x[1]) if isinstance(x[1], (int, float)) else 0, reverse=True)\n",
    "\n",
    "for i, (name, size) in enumerate(llm_list_sorted, 1):\n",
    "    marker = \" <- LARGEST\" if i == 1 else \"\"\n",
    "    print(f\"{i}. {name}: {size}B parameters{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Routing\n",
    "\n",
    "LargestLLM always routes to the largest model, regardless of query complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_QUERIES = [\n",
    "    {\"query\": \"What is 2 + 2?\"},  # Simple\n",
    "    {\"query\": \"Explain the theory of general relativity.\"},  # Medium\n",
    "    {\"query\": \"Prove P != NP.\"},  # Complex\n",
    "]\n",
    "\n",
    "print(\"Routing Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(EXAMPLE_QUERIES, 1):\n",
    "    result = router.route_single(query)\n",
    "    print(f\"{i}. {query['query'][:50]}...\")\n",
    "    print(f\"   Routed to: {result['model_name']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route test data\n",
    "test_queries = router.routing_data_test[:10]\n",
    "\n",
    "print(f\"Routing {len(test_queries)} test queries...\")\n",
    "results = router.route(test_queries)\n",
    "\n",
    "print(f\"\\nRouting Distribution:\")\n",
    "from collections import Counter\n",
    "model_counts = Counter([r['model_name'] for r in results])\n",
    "for model, count in model_counts.most_common():\n",
    "    print(f\"  {model}: {count} ({100*count/len(results):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(router=router)\n",
    "metrics = evaluator.eval()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for metric_name, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric_name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with SmallestLLM\n",
    "\n",
    "Compare the two baseline routers to understand the performance-cost tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.models.smallest_llm import SmallestLLMRouter\n",
    "\n",
    "smallest_router = SmallestLLMRouter(yaml_path=\"configs/model_config_train/smallest_llm.yaml\")\n",
    "smallest_evaluator = Evaluator(router=smallest_router)\n",
    "smallest_metrics = smallest_evaluator.eval()\n",
    "\n",
    "print(\"Comparison: SmallestLLM vs LargestLLM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<20} {'SmallestLLM':<15} {'LargestLLM':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for key in metrics.keys():\n",
    "    if key in smallest_metrics:\n",
    "        s_val = smallest_metrics[key]\n",
    "        l_val = metrics[key]\n",
    "        if isinstance(s_val, float):\n",
    "            print(f\"{key:<20} {s_val:<15.4f} {l_val:<15.4f}\")\n",
    "        else:\n",
    "            print(f\"{key:<20} {str(s_val):<15} {str(l_val):<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. File-Based Inference\n\nLoad queries from a custom file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from your own query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/largest_llm_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries from file\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries using route_batch\n    file_results = router.route_batch(batch=file_queries[:10])\n    print(f\"Routed {len(file_results)} queries\")\n    \n    # Save results to file\n    save_results_to_file(file_results, OUTPUT_FILE)\n    \n    # Show sample results\n    print(f\"\\nSample results:\")\n    for i, result in enumerate(file_results[:3], 1):\n        print(f\"  {i}. {result.get('query', '')[:40]}... -> {result['model_name']}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**LargestLLM Router**:\n",
    "- Always routes to the largest model\n",
    "- No training required (deterministic baseline)\n",
    "- Provides upper bound for performance, lower bound for cost efficiency\n",
    "- Useful for comparing against learned routing methods\n",
    "\n",
    "**Use Cases**:\n",
    "- Baseline comparison for routing research\n",
    "- Quality-critical applications regardless of cost\n",
    "- Establishing performance ceiling for routing methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}