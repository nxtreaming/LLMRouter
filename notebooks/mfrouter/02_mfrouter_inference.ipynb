{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFRouter - Inference\n",
    "\n",
    "This notebook demonstrates how to use a trained **MFRouter** for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.models.mfrouter import MFRouter\n",
    "from llmrouter.utils import setup_environment, load_model\n",
    "import yaml\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/model_config_train/mfrouter.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['model_path']['load_model_path'] = config['model_path']['save_model_path']\n",
    "\n",
    "INFERENCE_CONFIG_PATH = \"configs/model_config_test/mfrouter_inference.yaml\"\n",
    "os.makedirs(os.path.dirname(INFERENCE_CONFIG_PATH), exist_ok=True)\n",
    "\n",
    "with open(INFERENCE_CONFIG_PATH, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "router = MFRouter(yaml_path=INFERENCE_CONFIG_PATH)\n",
    "print(f\"Router loaded with {len(router.llm_data)} LLM candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_QUERIES = [\n",
    "    {\"query\": \"What is the capital of France?\"},\n",
    "    {\"query\": \"Solve the equation: 2x + 5 = 15\"},\n",
    "    {\"query\": \"Write a Python function to check if a number is prime.\"},\n",
    "    {\"query\": \"Explain quantum computing in simple terms.\"},\n",
    "]\n",
    "\n",
    "print(\"Routing Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(EXAMPLE_QUERIES, 1):\n",
    "    result = router.route_single(query)\n",
    "    print(f\"{i}. {query['query'][:50]}...\")\n",
    "    print(f\"   Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4. File-Based Inference\n\nLoad queries from a file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from default query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/mfrouter_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries\n    file_results = router.route_batch(batch=file_queries[:10])\n    print(f\"Routed {len(file_results)} queries\")\n    \n    # Save results\n    save_results_to_file(file_results, OUTPUT_FILE)\n    \n    # Show sample results\n    print(f\"\\nSample results:\")\n    for i, result in enumerate(file_results[:3], 1):\n        print(f\"  {i}. {result.get('query', '')[:40]}... -> {result['model_name']}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading a trained MFRouter\n",
    "2. Routing queries using matrix factorization\n",
    "\n",
    "MFRouter is effective for:\n",
    "- Capturing query-model interaction patterns\n",
    "- Collaborative filtering-style routing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}