{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMMultiRoundRouter - Inference\n",
    "\n",
    "This notebook demonstrates how to use the **LLMMultiRoundRouter** for multi-round query processing.\n",
    "\n",
    "## Overview\n",
    "\n",
    "LLMMultiRoundRouter uses LLM prompts for both query decomposition and routing decisions.\n",
    "Unlike KNN-based routers, it doesn't require training - it uses LLM reasoning directly.\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Decompose + Route**: LLM breaks query into sub-queries AND routes each in one step\n",
    "2. **Execute**: Call routed model APIs for each sub-query\n",
    "3. **Aggregate**: LLM combines responses into final answer\n",
    "\n",
    "**Key Features**:\n",
    "- No training required (LLM-based reasoning)\n",
    "- Single-step decomposition and routing\n",
    "- Model descriptions guide routing decisions\n",
    "- Supports both local vLLM and API-based inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "import os\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !git clone https://github.com/ulab-uiuc/LLMRouter.git\n",
    "    %cd LLMRouter\n",
    "    !pip install -e .\n",
    "    !pip install pyyaml openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.utils import setup_environment\n",
    "import yaml\n",
    "\n",
    "setup_environment()\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "LLMMultiRoundRouter requires:\n",
    "\n",
    "| Parameter | Description | Required |\n",
    "|-----------|-------------|----------|\n",
    "| `llm_data` | LLM candidates with descriptions | Yes |\n",
    "| `base_model` | LLM for decomposition/aggregation | Yes |\n",
    "| `api_endpoint` | API endpoint for execution | Yes |\n",
    "| `use_local_llm` | Use vLLM for local inference | No |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for LLMMultiRoundRouter\n",
    "llm_multi_config = {\n",
    "    \"data_path\": {\n",
    "        \"query_data_test\": \"data/example_data/query_data/default_query_test.jsonl\",\n",
    "        \"routing_data_test\": \"data/example_data/routing_data/default_routing_test_data.jsonl\",\n",
    "        \"llm_data\": \"data/example_data/llm_candidates/default_llm.json\"\n",
    "    },\n",
    "    \"base_model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"use_local_llm\": False,\n",
    "    \"api_endpoint\": os.environ.get(\"API_ENDPOINT\", \"https://api.openai.com/v1\")\n",
    "}\n",
    "\n",
    "# Save config\n",
    "CONFIG_PATH = \"configs/model_config_train/llmmultiroundrouter_temp.yaml\"\n",
    "os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n",
    "\n",
    "with open(CONFIG_PATH, 'w') as f:\n",
    "    yaml.dump(llm_multi_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(yaml.dump(llm_multi_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmrouter.models.llmmultiroundrouter import LLMMultiRoundRouter\n",
    "\n",
    "try:\n",
    "    router = LLMMultiRoundRouter(yaml_path=CONFIG_PATH)\n",
    "    print(\"Router initialized successfully!\")\n",
    "    print(f\"Base model: {router.base_model}\")\n",
    "    print(f\"Use local LLM: {router.use_local_llm}\")\n",
    "    print(f\"Number of LLM candidates: {len(router.llm_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing router: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display LLM candidates with their descriptions\n",
    "print(\"Available LLM Candidates:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, info in router.llm_data.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    if 'description' in info:\n",
    "        print(f\"  Description: {info['description'][:100]}...\")\n",
    "    if 'size' in info:\n",
    "        print(f\"  Size: {info['size']}B parameters\")\n",
    "    if 'capabilities' in info:\n",
    "        print(f\"  Capabilities: {info['capabilities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat Mode (Simple Queries)\n",
    "\n",
    "For simple queries, pass a string and get a string response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat mode - simple string input/output\n",
    "query = \"What are the main causes of climate change and what solutions exist?\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    response = router.route_single(query)\n",
    "    print(f\"\\nResponse:\\n{response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nNote: LLMMultiRoundRouter requires API access for LLM calls.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Mode (With Metrics)\n",
    "\n",
    "For evaluation, pass a dict with task_name and ground_truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode - dict input with metrics\n",
    "eval_query = {\n",
    "    \"query\": \"What is the largest planet in our solar system?\",\n",
    "    \"task_name\": \"trivia\",\n",
    "    \"ground_truth\": \"Jupiter\"\n",
    "}\n",
    "\n",
    "print(f\"Query: {eval_query['query']}\")\n",
    "print(f\"Task: {eval_query['task_name']}\")\n",
    "print(f\"Ground Truth: {eval_query['ground_truth']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = router.route_single(eval_query)\n",
    "    \n",
    "    print(f\"\\nResponse: {result.get('response', 'N/A')}\")\n",
    "    print(f\"Success: {result.get('success', False)}\")\n",
    "    print(f\"Prompt Tokens: {result.get('prompt_tokens', 0)}\")\n",
    "    print(f\"Completion Tokens: {result.get('completion_tokens', 0)}\")\n",
    "    if 'task_performance' in result:\n",
    "        print(f\"Task Performance: {result['task_performance']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "batch_queries = [\n",
    "    {\"query\": \"Explain quantum computing.\"},\n",
    "    {\"query\": \"What is the difference between AI and ML?\"},\n",
    "    {\"query\": \"How does blockchain technology work?\"},\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(batch_queries)} queries...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    results = router.route_batch(batch_queries)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Query: {result.get('query', 'N/A')[:50]}...\")\n",
    "        print(f\"   Success: {result.get('success', False)}\")\n",
    "        response = result.get('response', 'N/A')\n",
    "        print(f\"   Response: {response[:100] if response else 'N/A'}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task-Specific Routing\n",
    "\n",
    "LLMMultiRoundRouter supports task-specific prompts for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple choice task\n",
    "mc_query = {\n",
    "    \"query\": \"What is the capital of Australia? A) Sydney B) Melbourne C) Canberra D) Perth\",\n",
    "    \"task_name\": \"commonsense_qa\",\n",
    "    \"choices\": [\"Sydney\", \"Melbourne\", \"Canberra\", \"Perth\"],\n",
    "    \"ground_truth\": \"C\"\n",
    "}\n",
    "\n",
    "print(f\"Multiple Choice Query:\")\n",
    "print(f\"Question: {mc_query['query']}\")\n",
    "print(f\"Ground Truth: {mc_query['ground_truth']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    results = router.route_batch([mc_query], task_name=\"commonsense_qa\")\n",
    "    result = results[0]\n",
    "    \n",
    "    print(f\"\\nResponse: {result.get('response', 'N/A')}\")\n",
    "    if 'task_performance' in result:\n",
    "        print(f\"Correct: {result['task_performance'] == 1.0}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding the Pipeline\n",
    "\n",
    "Let's examine how LLMMultiRoundRouter works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLMMultiRoundRouter Pipeline:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Input Query                              │\n",
    "│  \"What are the causes of climate change and solutions?\"    │\n",
    "└────────────────────────┬────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Step 1: Decompose + Route                      │\n",
    "│  LLM breaks query into sub-queries AND routes each          │\n",
    "│                                                             │\n",
    "│  Output format: <sub-query>: <model-name>                   │\n",
    "│  • \"What causes climate change?\": Qwen-7B                   │\n",
    "│  • \"What solutions exist for climate change?\": Llama-70B    │\n",
    "└────────────────────────┬────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Step 2: Execute Sub-queries                    │\n",
    "│  Call routed model API for each sub-query                   │\n",
    "│                                                             │\n",
    "│  • Qwen-7B → \"Greenhouse gases, deforestation...\"           │\n",
    "│  • Llama-70B → \"Renewable energy, carbon capture...\"        │\n",
    "└────────────────────────┬────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│              Step 3: Aggregate Responses                    │\n",
    "│  LLM combines sub-responses into coherent final answer      │\n",
    "│                                                             │\n",
    "│  \"Climate change is caused by greenhouse gases...           │\n",
    "│   Solutions include renewable energy and...\"                │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nKey Advantages:\")\n",
    "print(\"• No training required - uses LLM reasoning directly\")\n",
    "print(\"• Model descriptions guide intelligent routing decisions\")\n",
    "print(\"• Single-step decomposition and routing for efficiency\")\n",
    "print(\"• Flexible aggregation based on task type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with KNNMultiRoundRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLMMultiRoundRouter vs KNNMultiRoundRouter:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = \"\"\"\n",
    "| Feature              | LLMMultiRoundRouter     | KNNMultiRoundRouter     |\n",
    "|----------------------|-------------------------|-------------------------|\n",
    "| Training Required    | No                      | Yes (KNN fitting)       |\n",
    "| Routing Method       | LLM reasoning           | KNN on embeddings       |\n",
    "| Model Selection      | Based on descriptions   | Based on similarity     |\n",
    "| Decomposition        | Same LLM call           | Separate LLM call       |\n",
    "| Flexibility          | High (prompt-based)     | Medium (learned)        |\n",
    "| Inference Cost       | Higher (more LLM calls) | Lower (KNN is fast)     |\n",
    "| Cold Start           | Works immediately       | Needs training data     |\n",
    "\"\"\"\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 10. File-Based Inference\n\nLoad queries from a file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from default query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/llmmultiroundrouter_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries (limit to 5 for demo due to API costs)\n    try:\n        file_results = router.route_batch(file_queries[:5])\n        print(f\"Routed {len(file_results)} queries\")\n        \n        # Save results\n        save_results_to_file(file_results, OUTPUT_FILE)\n        \n        # Show sample results\n        print(f\"\\nSample results:\")\n        for i, result in enumerate(file_results[:3], 1):\n            print(f\"  {i}. {result.get('query', '')[:40]}...\")\n            print(f\"     Success: {result.get('success', False)}\")\n    except Exception as e:\n        print(f\"Error during batch routing: {e}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**LLMMultiRoundRouter** provides:\n",
    "- Zero-shot multi-round routing (no training)\n",
    "- LLM-based decomposition and routing in one step\n",
    "- Model description-guided routing decisions\n",
    "- Flexible aggregation for different task types\n",
    "\n",
    "**Use Cases**:\n",
    "- Quick prototyping without training data\n",
    "- Complex queries requiring expert routing decisions\n",
    "- When model descriptions are more reliable than embeddings\n",
    "- Low-volume, high-quality routing needs\n",
    "\n",
    "**Requirements**:\n",
    "- API access for LLM calls\n",
    "- Optional: vLLM for local inference\n",
    "- Model descriptions in llm_data (recommended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}