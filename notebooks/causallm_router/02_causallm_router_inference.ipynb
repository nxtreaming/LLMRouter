{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CausalLMRouter - Inference\n",
    "\n",
    "This notebook demonstrates how to use a trained **CausalLMRouter** for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llmrouter.models.causallm_router import CausalLMRouter\n",
    "from llmrouter.utils import setup_environment\n",
    "import yaml\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/model_config_train/causallm_router.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Use merged model for inference\n",
    "config['model_path']['load_model_path'] = config['model_path'].get(\n",
    "    'load_model_path', \n",
    "    os.path.join(config['model_path']['save_model_path'], 'merged')\n",
    ")\n",
    "\n",
    "router = CausalLMRouter(yaml_path=CONFIG_PATH)\n",
    "print(f\"Router loaded with {len(router.llm_data)} LLM candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_QUERIES = [\n",
    "    {\"query\": \"What is the capital of France?\"},\n",
    "    {\"query\": \"Solve the equation: 2x + 5 = 15\"},\n",
    "    {\"query\": \"Write a Python function to check if a number is prime.\"},\n",
    "    {\"query\": \"Explain the difference between supervised and unsupervised learning.\"},\n",
    "    {\"query\": \"What are the main causes of climate change?\"},\n",
    "]\n",
    "\n",
    "print(\"Routing Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, query in enumerate(EXAMPLE_QUERIES, 1):\n",
    "    result = router.route_single(query)\n",
    "    print(f\"{i}. {query['query'][:55]}...\")\n",
    "    print(f\"   Routed to: {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using vLLM for Fast Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM provides faster inference for CausalLM models\n",
    "# Install: pip install vllm\n",
    "\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    vllm_available = True\n",
    "    print(\"vLLM is available for accelerated inference\")\n",
    "except ImportError:\n",
    "    vllm_available = False\n",
    "    print(\"vLLM not installed. Using standard HuggingFace inference.\")\n",
    "    print(\"For faster inference, install vLLM: pip install vllm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5. File-Based Inference\n\nLoad queries from a file and save results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\n\n# Load queries from a JSONL file\ndef load_queries_from_file(file_path):\n    \"\"\"Load queries from a JSONL file.\"\"\"\n    queries = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                queries.append(json.loads(line))\n    return queries\n\n# Save results to a JSONL file\ndef save_results_to_file(results, output_path):\n    \"\"\"Save routing results to a JSONL file.\"\"\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for result in results:\n            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n    print(f\"Results saved to: {output_path}\")\n\n# Example: Load from default query file\nQUERY_FILE = \"data/example_data/query_data/default_query_test.jsonl\"\nOUTPUT_FILE = \"outputs/causallm_router_results.jsonl\"\n\nif os.path.exists(QUERY_FILE):\n    # Load queries\n    file_queries = load_queries_from_file(QUERY_FILE)\n    print(f\"Loaded {len(file_queries)} queries from: {QUERY_FILE}\")\n    \n    # Route queries\n    file_results = router.route_batch(batch=file_queries[:10])\n    print(f\"Routed {len(file_results)} queries\")\n    \n    # Save results\n    save_results_to_file(file_results, OUTPUT_FILE)\n    \n    # Show sample results\n    print(f\"\\nSample results:\")\n    for i, result in enumerate(file_results[:3], 1):\n        print(f\"  {i}. {result.get('query', '')[:40]}... -> {result['model_name']}\")\nelse:\n    print(f\"Query file not found: {QUERY_FILE}\")\n    print(\"Create a JSONL file with format: {\\\"query\\\": \\\"Your question\\\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading a trained CausalLMRouter\n",
    "2. Routing queries using LLM-based inference\n",
    "\n",
    "CausalLMRouter is effective for:\n",
    "- Complex queries requiring deep semantic understanding\n",
    "- High-quality routing with LLM reasoning\n",
    "\n",
    "**Tips for Production**:\n",
    "- Use vLLM for faster inference\n",
    "- Consider quantization (4-bit, 8-bit) for memory efficiency\n",
    "- Batch queries for better throughput"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}