{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Datasets for LLMRouter\n",
    "\n",
    "**Estimated Time:** 45 minutes  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** 00_Quick_Start, 02_Data_Preparation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ulab-uiuc/LLMRouter/blob/main/tutorials/notebooks/10_Creating_Custom_Datasets.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Understand all data formats in LLMRouter\n",
    "- ‚úÖ Create query datasets from scratch\n",
    "- ‚úÖ Create routing ground truth data\n",
    "- ‚úÖ Convert existing datasets (ChatBot Arena, MT-Bench, etc.)\n",
    "- ‚úÖ Create domain-specific datasets\n",
    "- ‚úÖ Validate data quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!git clone https://github.com/ulab-uiuc/LLMRouter.git\n",
    "%cd LLMRouter\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Data Formats\n",
    "\n",
    "LLMRouter uses three main data types:\n",
    "\n",
    "### 1.1 Query Data (JSONL)\n",
    "\n",
    "Simple list of queries, one per line:\n",
    "\n",
    "```jsonl\n",
    "{\"query\": \"What is machine learning?\", \"id\": \"q1\"}\n",
    "{\"query\": \"Explain quantum physics\", \"id\": \"q2\"}\n",
    "```\n",
    "\n",
    "### 1.2 Routing Data (JSONL)\n",
    "\n",
    "Ground truth for training - which model is best for each query:\n",
    "\n",
    "```jsonl\n",
    "{\"query\": \"What is ML?\", \"best_llm\": \"gpt-4\", \"performance\": 0.95}\n",
    "{\"query\": \"Code a sort\", \"best_llm\": \"code-llama\", \"performance\": 0.88}\n",
    "```\n",
    "\n",
    "### 1.3 LLM Candidates (JSON)\n",
    "\n",
    "Available models (covered in Tutorial 09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load example data\n",
    "with open('data/example_data/query_data/default_query_test.jsonl', 'r') as f:\n",
    "    queries = [json.loads(line) for line in f]\n",
    "\n",
    "with open('data/example_data/routing_data/default_routing_test_data.jsonl', 'r') as f:\n",
    "    routing_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Sample query: {queries[0]}\")\n",
    "print(f\"\\nSample routing data: {routing_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Query Dataset from Scratch\n",
    "\n",
    "Let's create a custom domain-specific dataset.\n",
    "\n",
    "**Example Domain:** Programming Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create programming queries dataset\n",
    "programming_queries = [\n",
    "    {\"query\": \"Write a Python function to reverse a string\", \"id\": \"prog_1\", \"category\": \"coding\"},\n",
    "    {\"query\": \"Explain the difference between == and === in JavaScript\", \"id\": \"prog_2\", \"category\": \"concept\"},\n",
    "    {\"query\": \"What is a binary search tree?\", \"id\": \"prog_3\", \"category\": \"theory\"},\n",
    "    {\"query\": \"Debug this code: for i in range(10) print(i)\", \"id\": \"prog_4\", \"category\": \"debugging\"},\n",
    "    {\"query\": \"How do I optimize SQL queries?\", \"id\": \"prog_5\", \"category\": \"optimization\"},\n",
    "    {\"query\": \"Implement quicksort in C++\", \"id\": \"prog_6\", \"category\": \"coding\"},\n",
    "    {\"query\": \"What are design patterns? Give examples\", \"id\": \"prog_7\", \"category\": \"architecture\"},\n",
    "    {\"query\": \"Convert this loop to list comprehension: result = []; for i in range(10): result.append(i*2)\", \"id\": \"prog_8\", \"category\": \"refactoring\"},\n",
    "]\n",
    "\n",
    "# Save to JSONL\n",
    "with open('my_programming_queries.jsonl', 'w') as f:\n",
    "    for q in programming_queries:\n",
    "        f.write(json.dumps(q) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created {len(programming_queries)} programming queries\")\n",
    "print(\"\\nCategories:\")\n",
    "categories = {}\n",
    "for q in programming_queries:\n",
    "    cat = q['category']\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "for cat, count in sorted(categories.items()):\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Routing Ground Truth\n",
    "\n",
    "### Method 1: Manual Labeling\n",
    "\n",
    "For small datasets, manually assign best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual routing labels\n",
    "# Assume we have these models: gpt-4, claude-3, code-llama, llama-3-8b\n",
    "\n",
    "routing_labels = [\n",
    "    {\"query\": \"Write a Python function to reverse a string\", \n",
    "     \"best_llm\": \"code-llama\", \n",
    "     \"performance\": 0.92,\n",
    "     \"reason\": \"code generation task\"},\n",
    "    \n",
    "    {\"query\": \"Explain the difference between == and === in JavaScript\", \n",
    "     \"best_llm\": \"gpt-4\", \n",
    "     \"performance\": 0.95,\n",
    "     \"reason\": \"requires clear explanation\"},\n",
    "    \n",
    "    {\"query\": \"What is a binary search tree?\", \n",
    "     \"best_llm\": \"claude-3\", \n",
    "     \"performance\": 0.90,\n",
    "     \"reason\": \"conceptual explanation\"},\n",
    "    \n",
    "    {\"query\": \"Debug this code: for i in range(10) print(i)\", \n",
    "     \"best_llm\": \"code-llama\", \n",
    "     \"performance\": 0.88,\n",
    "     \"reason\": \"code debugging\"},\n",
    "    \n",
    "    {\"query\": \"How do I optimize SQL queries?\", \n",
    "     \"best_llm\": \"gpt-4\", \n",
    "     \"performance\": 0.93,\n",
    "     \"reason\": \"complex technical topic\"},\n",
    "    \n",
    "    {\"query\": \"Implement quicksort in C++\", \n",
    "     \"best_llm\": \"code-llama\", \n",
    "     \"performance\": 0.94,\n",
    "     \"reason\": \"algorithm implementation\"},\n",
    "    \n",
    "    {\"query\": \"What are design patterns? Give examples\", \n",
    "     \"best_llm\": \"gpt-4\", \n",
    "     \"performance\": 0.91,\n",
    "     \"reason\": \"requires examples and explanation\"},\n",
    "    \n",
    "    {\"query\": \"Convert this loop to list comprehension: result = []; for i in range(10): result.append(i*2)\", \n",
    "     \"best_llm\": \"code-llama\", \n",
    "     \"performance\": 0.96,\n",
    "     \"reason\": \"code transformation\"},\n",
    "]\n",
    "\n",
    "# Save routing data\n",
    "with open('my_routing_labels.jsonl', 'w') as f:\n",
    "    for label in routing_labels:\n",
    "        f.write(json.dumps(label) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created {len(routing_labels)} routing labels\")\n",
    "\n",
    "# Analyze distribution\n",
    "model_counts = {}\n",
    "for label in routing_labels:\n",
    "    model = label['best_llm']\n",
    "    model_counts[model] = model_counts.get(model, 0) + 1\n",
    "\n",
    "print(\"\\nModel distribution:\")\n",
    "for model, count in sorted(model_counts.items()):\n",
    "    print(f\"  {model}: {count} queries ({count/len(routing_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Generate from Existing Evaluations\n",
    "\n",
    "If you have evaluation results from running multiple models on queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: You ran 4 models on each query and measured performance\n",
    "evaluation_results = [\n",
    "    {\n",
    "        \"query\": \"Write a Python function to reverse a string\",\n",
    "        \"results\": {\n",
    "            \"gpt-4\": {\"score\": 0.85, \"cost\": 0.01},\n",
    "            \"claude-3\": {\"score\": 0.83, \"cost\": 0.015},\n",
    "            \"code-llama\": {\"score\": 0.92, \"cost\": 0.002},\n",
    "            \"llama-3-8b\": {\"score\": 0.75, \"cost\": 0.0005},\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain quantum physics\",\n",
    "        \"results\": {\n",
    "            \"gpt-4\": {\"score\": 0.95, \"cost\": 0.01},\n",
    "            \"claude-3\": {\"score\": 0.93, \"cost\": 0.015},\n",
    "            \"code-llama\": {\"score\": 0.60, \"cost\": 0.002},\n",
    "            \"llama-3-8b\": {\"score\": 0.70, \"cost\": 0.0005},\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "def generate_routing_from_eval(evaluations, strategy='best_performance'):\n",
    "    \"\"\"Generate routing data from evaluation results.\n",
    "    \n",
    "    Strategies:\n",
    "    - best_performance: Choose model with highest score\n",
    "    - cost_aware: Balance score and cost\n",
    "    - threshold: Use cheapest model above threshold\n",
    "    \"\"\"\n",
    "    routing_data = []\n",
    "    \n",
    "    for eval_item in evaluations:\n",
    "        query = eval_item['query']\n",
    "        results = eval_item['results']\n",
    "        \n",
    "        if strategy == 'best_performance':\n",
    "            # Select model with highest score\n",
    "            best_model = max(results.items(), key=lambda x: x[1]['score'])\n",
    "            routing_data.append({\n",
    "                'query': query,\n",
    "                'best_llm': best_model[0],\n",
    "                'performance': best_model[1]['score'],\n",
    "                'cost': best_model[1]['cost'],\n",
    "            })\n",
    "        \n",
    "        elif strategy == 'cost_aware':\n",
    "            # Maximize score/cost ratio\n",
    "            best_model = max(results.items(), \n",
    "                           key=lambda x: x[1]['score'] / max(x[1]['cost'], 0.0001))\n",
    "            routing_data.append({\n",
    "                'query': query,\n",
    "                'best_llm': best_model[0],\n",
    "                'performance': best_model[1]['score'],\n",
    "                'cost': best_model[1]['cost'],\n",
    "                'strategy': 'cost_aware',\n",
    "            })\n",
    "    \n",
    "    return routing_data\n",
    "\n",
    "# Generate routing data\n",
    "auto_routing = generate_routing_from_eval(evaluation_results, 'best_performance')\n",
    "\n",
    "print(\"Generated routing data:\")\n",
    "for item in auto_routing:\n",
    "    print(f\"  {item['query'][:40]}... ‚Üí {item['best_llm']} (score: {item['performance']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Existing Datasets\n",
    "\n",
    "### 4.1 From ChatBot Arena Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ChatBot Arena data (conversation format)\n",
    "chatbot_arena_data = [\n",
    "    {\n",
    "        \"conversation\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Neural networks are...\"},\n",
    "        ],\n",
    "        \"model_a\": \"gpt-4\",\n",
    "        \"model_b\": \"claude-3\",\n",
    "        \"winner\": \"model_a\",  # or \"model_b\" or \"tie\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def convert_chatbot_arena(arena_data):\n",
    "    \"\"\"Convert ChatBot Arena format to LLMRouter format.\"\"\"\n",
    "    query_data = []\n",
    "    routing_data = []\n",
    "    \n",
    "    for i, item in enumerate(arena_data):\n",
    "        # Extract first user message as query\n",
    "        query = next((msg['content'] for msg in item['conversation'] \n",
    "                     if msg['role'] == 'user'), None)\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        # Add to query dataset\n",
    "        query_data.append({\n",
    "            'query': query,\n",
    "            'id': f'arena_{i}',\n",
    "        })\n",
    "        \n",
    "        # Determine best model from winner\n",
    "        if item['winner'] == 'model_a':\n",
    "            best_model = item['model_a']\n",
    "            performance = 1.0\n",
    "        elif item['winner'] == 'model_b':\n",
    "            best_model = item['model_b']\n",
    "            performance = 1.0\n",
    "        else:  # tie\n",
    "            best_model = item['model_a']  # arbitrary choice\n",
    "            performance = 0.5\n",
    "        \n",
    "        routing_data.append({\n",
    "            'query': query,\n",
    "            'best_llm': best_model,\n",
    "            'performance': performance,\n",
    "            'source': 'chatbot_arena',\n",
    "        })\n",
    "    \n",
    "    return query_data, routing_data\n",
    "\n",
    "queries, routing = convert_chatbot_arena(chatbot_arena_data)\n",
    "print(f\"‚úÖ Converted {len(queries)} ChatBot Arena examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 From MT-Bench Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MT-Bench format (multi-turn benchmark)\n",
    "mt_bench_data = [\n",
    "    {\n",
    "        \"question_id\": 1,\n",
    "        \"category\": \"writing\",\n",
    "        \"turns\": [\n",
    "            \"Write a short story about AI\",\n",
    "            \"Now make it a poem\",\n",
    "        ],\n",
    "        \"reference_answer\": \"...\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def convert_mt_bench(mt_bench_data):\n",
    "    \"\"\"Convert MT-Bench to LLMRouter format.\"\"\"\n",
    "    query_data = []\n",
    "    \n",
    "    for item in mt_bench_data:\n",
    "        # Use first turn as query\n",
    "        query_data.append({\n",
    "            'query': item['turns'][0],\n",
    "            'id': f\"mtbench_{item['question_id']}\",\n",
    "            'category': item['category'],\n",
    "            'multi_turn': len(item['turns']) > 1,\n",
    "        })\n",
    "    \n",
    "    return query_data\n",
    "\n",
    "queries = convert_mt_bench(mt_bench_data)\n",
    "print(f\"‚úÖ Converted {len(queries)} MT-Bench examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Validation\n",
    "\n",
    "Always validate your dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(query_file, routing_file, llm_file):\n",
    "    \"\"\"Validate dataset consistency and quality.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    with open(query_file, 'r') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "    \n",
    "    with open(routing_file, 'r') as f:\n",
    "        routing = [json.loads(line) for line in f]\n",
    "    \n",
    "    with open(llm_file, 'r') as f:\n",
    "        llm_data = json.load(f)\n",
    "    \n",
    "    print(\"üìä Dataset Validation Report\\n\" + \"=\"*60)\n",
    "    \n",
    "    # 1. Check sizes\n",
    "    print(f\"\\n1. Dataset Sizes:\")\n",
    "    print(f\"   Queries: {len(queries)}\")\n",
    "    print(f\"   Routing labels: {len(routing)}\")\n",
    "    print(f\"   LLM models: {len(llm_data)}\")\n",
    "    \n",
    "    if len(queries) != len(routing):\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Query count != Routing count\")\n",
    "    \n",
    "    # 2. Check required fields\n",
    "    print(f\"\\n2. Field Validation:\")\n",
    "    for i, q in enumerate(queries[:5]):\n",
    "        if 'query' not in q:\n",
    "            print(f\"   ‚ùå Query {i} missing 'query' field\")\n",
    "    \n",
    "    for i, r in enumerate(routing[:5]):\n",
    "        if 'query' not in r or 'best_llm' not in r:\n",
    "            print(f\"   ‚ùå Routing {i} missing required fields\")\n",
    "    \n",
    "    print(\"   ‚úÖ All required fields present\")\n",
    "    \n",
    "    # 3. Check model names\n",
    "    print(f\"\\n3. Model Name Validation:\")\n",
    "    used_models = set(r['best_llm'] for r in routing)\n",
    "    available_models = set(llm_data.keys())\n",
    "    \n",
    "    missing = used_models - available_models\n",
    "    if missing:\n",
    "        print(f\"   ‚ùå Models in routing but not in LLM data: {missing}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All models are available\")\n",
    "    \n",
    "    # 4. Distribution analysis\n",
    "    print(f\"\\n4. Model Distribution:\")\n",
    "    model_counts = {}\n",
    "    for r in routing:\n",
    "        model = r['best_llm']\n",
    "        model_counts[model] = model_counts.get(model, 0) + 1\n",
    "    \n",
    "    for model in sorted(model_counts.keys()):\n",
    "        count = model_counts[model]\n",
    "        pct = count / len(routing) * 100\n",
    "        print(f\"   {model}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # 5. Query length analysis\n",
    "    print(f\"\\n5. Query Length Statistics:\")\n",
    "    lengths = [len(q['query'].split()) for q in queries]\n",
    "    print(f\"   Min: {min(lengths)} words\")\n",
    "    print(f\"   Max: {max(lengths)} words\")\n",
    "    print(f\"   Mean: {sum(lengths)/len(lengths):.1f} words\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Validation complete\")\n",
    "\n",
    "# Run validation\n",
    "# validate_dataset(\n",
    "#     'my_programming_queries.jsonl',\n",
    "#     'my_routing_labels.jsonl',\n",
    "#     'data/example_data/llm_candidates/default_llm.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_train_test_split(data, test_ratio=0.2, random_seed=42):\n",
    "    \"\"\"Split data into train and test sets.\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffled = data.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    # Split\n",
    "    split_idx = int(len(shuffled) * (1 - test_ratio))\n",
    "    train = shuffled[:split_idx]\n",
    "    test = shuffled[split_idx:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Example\n",
    "all_data = routing_labels  # from earlier\n",
    "train_data, test_data = create_train_test_split(all_data, test_ratio=0.2)\n",
    "\n",
    "# Save splits\n",
    "with open('train_routing.jsonl', 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "with open('test_routing.jsonl', 'w') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Split complete:\")\n",
    "print(f\"   Train: {len(train_data)} examples\")\n",
    "print(f\"   Test: {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Synthetic Data Generation\n",
    "\n",
    "Generate synthetic queries using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires an LLM API\n",
    "# Example template for generating queries\n",
    "\n",
    "generation_prompt = \"\"\"\n",
    "Generate 10 diverse programming questions that vary in:\n",
    "- Difficulty (beginner to advanced)\n",
    "- Topic (algorithms, debugging, concepts, etc.)\n",
    "- Length (short to long)\n",
    "\n",
    "Format each as:\n",
    "{\"query\": \"...\", \"difficulty\": \"...\", \"topic\": \"...\"}\n",
    "\n",
    "One per line.\n",
    "\"\"\"\n",
    "\n",
    "# You would call an LLM API here\n",
    "# generated_queries = call_llm(generation_prompt)\n",
    "\n",
    "print(\"Example synthetic generation prompt:\")\n",
    "print(generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices\n",
    "\n",
    "### Dataset Size Guidelines\n",
    "\n",
    "- **Minimum**: 100 training examples\n",
    "- **Good**: 500-1000 examples\n",
    "- **Excellent**: 5000+ examples\n",
    "\n",
    "### Quality > Quantity\n",
    "\n",
    "‚úÖ **Good practices:**\n",
    "- Diverse query types\n",
    "- Balanced model distribution\n",
    "- Clear routing decisions\n",
    "- Real-world representative\n",
    "\n",
    "‚ùå **Avoid:**\n",
    "- Duplicate queries\n",
    "- Biased distributions\n",
    "- Ambiguous labels\n",
    "- Out-of-domain queries\n",
    "\n",
    "### Model Coverage\n",
    "\n",
    "Each model should have:\n",
    "- At least 50-100 examples\n",
    "- Examples showcasing its strengths\n",
    "- Coverage across difficulty levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ All LLMRouter data formats\n",
    "- ‚úÖ Creating query datasets\n",
    "- ‚úÖ Creating routing ground truth\n",
    "- ‚úÖ Converting existing datasets\n",
    "- ‚úÖ Data validation\n",
    "- ‚úÖ Train/test splitting\n",
    "- ‚úÖ Best practices\n",
    "\n",
    "### Key Files Created:\n",
    "1. `my_programming_queries.jsonl` - Custom queries\n",
    "2. `my_routing_labels.jsonl` - Routing ground truth\n",
    "3. `train_routing.jsonl` - Training split\n",
    "4. `test_routing.jsonl` - Test split\n",
    "\n",
    "### Next Steps:\n",
    "- **[03_Training_Single_Round_Routers.ipynb](03_Training_Single_Round_Routers.ipynb)** - Train with your data\n",
    "- **[05_Inference_and_Evaluation.ipynb](05_Inference_and_Evaluation.ipynb)** - Evaluate performance\n",
    "- **[11_Advanced_Customization.ipynb](11_Advanced_Customization.ipynb)** - Advanced techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
