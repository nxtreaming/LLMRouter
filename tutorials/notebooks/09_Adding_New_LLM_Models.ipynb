{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding New LLM Models to LLMRouter\n",
    "\n",
    "**Estimated Time:** 30 minutes  \n",
    "**Level:** Advanced  \n",
    "**Prerequisites:** 00_Quick_Start, 01_Installation_and_Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ulab-uiuc/LLMRouter/blob/main/tutorials/notebooks/09_Adding_New_LLM_Models.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- ‚úÖ Understand LLM candidates format\n",
    "- ‚úÖ Add new models to the router pool\n",
    "- ‚úÖ Generate model embeddings\n",
    "- ‚úÖ Configure API endpoints\n",
    "- ‚úÖ Test new models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!git clone https://github.com/ulab-uiuc/LLMRouter.git\n",
    "%cd LLMRouter\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding LLM Candidates Format\n",
    "\n",
    "LLM candidates are defined in JSON format. Let's examine the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load existing LLM data\n",
    "with open('data/example_data/llm_candidates/default_llm.json', 'r') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "# Show one example\n",
    "example_key = list(llm_data.keys())[0]\n",
    "print(f\"Example LLM: {example_key}\")\n",
    "print(json.dumps(llm_data[example_key], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Fields:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model_name\": {                    // User-friendly name\n",
    "    \"model\": \"provider/model-id\",    // API model identifier\n",
    "    \"size\": \"7B\",                    // Model size (optional)\n",
    "    \"cost\": 0.001,                   // Cost per 1K tokens (optional)\n",
    "    \"description\": \"...\",            // Model description (for embeddings)\n",
    "    \"embedding\": [0.1, 0.2, ...]     // Model embedding vector (optional)\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding a New Model - Method 1: Manual Entry\n",
    "\n",
    "Let's add a new model to the existing pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the LLM data\n",
    "import copy\n",
    "new_llm_data = copy.deepcopy(llm_data)\n",
    "\n",
    "# Add a new model\n",
    "new_llm_data[\"gpt-4-turbo\"] = {\n",
    "    \"model\": \"openai/gpt-4-turbo\",\n",
    "    \"size\": \"Unknown\",\n",
    "    \"cost\": 0.01,  # $0.01 per 1K tokens\n",
    "    \"description\": \"GPT-4 Turbo is OpenAI's most advanced model with 128K context window, optimized for speed and cost.\"\n",
    "}\n",
    "\n",
    "new_llm_data[\"claude-3-opus\"] = {\n",
    "    \"model\": \"anthropic/claude-3-opus\",\n",
    "    \"size\": \"Unknown\",\n",
    "    \"cost\": 0.015,\n",
    "    \"description\": \"Claude 3 Opus is Anthropic's most capable model with strong reasoning and analysis capabilities.\"\n",
    "}\n",
    "\n",
    "new_llm_data[\"llama-3-70b\"] = {\n",
    "    \"model\": \"meta/llama-3-70b-instruct\",\n",
    "    \"size\": \"70B\",\n",
    "    \"cost\": 0.0005,\n",
    "    \"description\": \"Llama 3 70B is Meta's open-source large language model with strong performance across tasks.\"\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Added {len(new_llm_data) - len(llm_data)} new models\")\n",
    "print(f\"Total models: {len(new_llm_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Model Embeddings\n",
    "\n",
    "Some routers (e.g., KNN, Graph) use model embeddings to understand model capabilities.\n",
    "\n",
    "**Two approaches:**\n",
    "1. **Use model descriptions** (recommended) - Generate from text descriptions\n",
    "2. **Manual embeddings** - If you have pre-computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Generate embeddings from descriptions\n",
    "from llmrouter.utils import get_longformer_embedding\n",
    "\n",
    "def add_embeddings_to_llm_data(llm_data):\n",
    "    \"\"\"Generate embeddings for LLMs based on their descriptions.\"\"\"\n",
    "    llm_data_with_embeddings = copy.deepcopy(llm_data)\n",
    "    \n",
    "    for model_name, model_info in llm_data_with_embeddings.items():\n",
    "        # Skip if embedding already exists\n",
    "        if 'embedding' in model_info:\n",
    "            print(f\"‚úì {model_name}: embedding exists\")\n",
    "            continue\n",
    "        \n",
    "        # Get description\n",
    "        description = model_info.get('description', '')\n",
    "        if not description:\n",
    "            # Create basic description from available info\n",
    "            description = f\"{model_name} is a language model\"\n",
    "            if 'size' in model_info:\n",
    "                description += f\" with {model_info['size']} parameters\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        print(f\"üîÑ Generating embedding for {model_name}...\")\n",
    "        embedding = get_longformer_embedding(description)\n",
    "        \n",
    "        # Convert to list for JSON serialization\n",
    "        model_info['embedding'] = embedding.tolist()\n",
    "        print(f\"‚úÖ {model_name}: embedding generated ({len(embedding)} dims)\")\n",
    "    \n",
    "    return llm_data_with_embeddings\n",
    "\n",
    "# Generate embeddings\n",
    "new_llm_data_with_embeddings = add_embeddings_to_llm_data(new_llm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the Updated LLM Data\n",
    "\n",
    "Save the new LLM configuration to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to new file\n",
    "output_path = 'data/example_data/llm_candidates/my_custom_llm.json'\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(new_llm_data_with_embeddings, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {output_path}\")\n",
    "print(f\"Total models: {len(new_llm_data_with_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Custom Configuration\n",
    "\n",
    "Create a router configuration that uses your new LLM pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create configuration\n",
    "config = {\n",
    "    'data_path': {\n",
    "        'llm_data': 'data/example_data/llm_candidates/my_custom_llm.json',\n",
    "        'query_data_test': 'data/example_data/query_data/default_query_test.jsonl',\n",
    "        'routing_data_test': 'data/example_data/routing_data/default_routing_test_data.jsonl',\n",
    "    },\n",
    "    'metric': {\n",
    "        'weights': {\n",
    "            'performance': 1,\n",
    "            'cost': 0,\n",
    "            'llm_judge': 0,\n",
    "        }\n",
    "    },\n",
    "    'api_endpoint': 'https://integrate.api.nvidia.com/v1',\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = 'my_custom_config.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(f\"‚úÖ Created config: {config_path}\")\n",
    "!cat {config_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing with New Models\n",
    "\n",
    "Let's test routing with the new LLM pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with smallest_llm router (doesn't require training)\n",
    "!llmrouter infer \\\n",
    "  --router smallest_llm \\\n",
    "  --config my_custom_config.yaml \\\n",
    "  --query \"Explain machine learning\" \\\n",
    "  --route-only \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. API Integration\n",
    "\n",
    "To actually use these models, you need to configure API access.\n",
    "\n",
    "**Supported Providers (via LiteLLM):**\n",
    "- OpenAI (GPT-4, GPT-3.5, etc.)\n",
    "- Anthropic (Claude)\n",
    "- Google (Gemini)\n",
    "- Meta/HuggingFace (Llama, Mistral)\n",
    "- NVIDIA NIM\n",
    "- Custom OpenAI-compatible endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set API keys (use Colab secrets)\n",
    "# You can also use multiple keys for load balancing\n",
    "\n",
    "# Single key:\n",
    "# os.environ['API_KEYS'] = 'your-api-key'\n",
    "\n",
    "# Multiple keys (JSON format):\n",
    "# os.environ['API_KEYS'] = '[\"key1\", \"key2\", \"key3\"]'\n",
    "\n",
    "# Example using Colab secrets:\n",
    "try:\n",
    "    api_key = userdata.get('NVIDIA_API_KEY')\n",
    "    os.environ['API_KEYS'] = api_key\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è No API key found. Set NVIDIA_API_KEY in Colab secrets.\")\n",
    "    print(\"   Or manually: os.environ['API_KEYS'] = 'your-key'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real Inference with New Models\n",
    "\n",
    "Now let's actually call the LLM APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run real inference (will call API)\n",
    "# WARNING: This will use API credits!\n",
    "\n",
    "!llmrouter infer \\\n",
    "  --router smallest_llm \\\n",
    "  --config my_custom_config.yaml \\\n",
    "  --query \"What is 2+2?\" \\\n",
    "  --max-tokens 50 \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Model Metadata\n",
    "\n",
    "You can add custom metadata to help routers make better decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with extended metadata\n",
    "advanced_llm_data = {\n",
    "    \"specialized-coder\": {\n",
    "        \"model\": \"provider/code-model\",\n",
    "        \"size\": \"13B\",\n",
    "        \"cost\": 0.002,\n",
    "        \"description\": \"Specialized in code generation and debugging\",\n",
    "        \n",
    "        # Custom metadata\n",
    "        \"capabilities\": [\"code\", \"debugging\", \"algorithms\"],\n",
    "        \"languages\": [\"python\", \"javascript\", \"java\", \"c++\"],\n",
    "        \"context_length\": 8192,\n",
    "        \"latency_ms\": 500,\n",
    "        \"strengths\": \"code generation\",\n",
    "        \"weaknesses\": \"creative writing\",\n",
    "    },\n",
    "    \"general-assistant\": {\n",
    "        \"model\": \"provider/general-model\",\n",
    "        \"size\": \"70B\",\n",
    "        \"cost\": 0.005,\n",
    "        \"description\": \"General-purpose assistant with broad knowledge\",\n",
    "        \n",
    "        \"capabilities\": [\"qa\", \"writing\", \"analysis\", \"reasoning\"],\n",
    "        \"context_length\": 32768,\n",
    "        \"latency_ms\": 1200,\n",
    "        \"strengths\": \"reasoning and analysis\",\n",
    "        \"weaknesses\": \"highly specialized tasks\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# This metadata can be used by custom routers!\n",
    "print(json.dumps(advanced_llm_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "### Model Selection Criteria\n",
    "\n",
    "When adding models, consider:\n",
    "\n",
    "1. **Diversity**: Include models with different strengths\n",
    "   - Small fast models (e.g., 7B)\n",
    "   - Large capable models (e.g., 70B)\n",
    "   - Specialized models (code, math, etc.)\n",
    "\n",
    "2. **Cost Range**: Mix of expensive and cheap models\n",
    "   - Budget-friendly: < $0.001/1K tokens\n",
    "   - Mid-range: $0.001-$0.01/1K tokens  \n",
    "   - Premium: > $0.01/1K tokens\n",
    "\n",
    "3. **Latency**: Balance speed and quality\n",
    "   - Fast: < 500ms\n",
    "   - Medium: 500-2000ms\n",
    "   - Slow: > 2000ms\n",
    "\n",
    "### Description Guidelines\n",
    "\n",
    "Good descriptions help routers learn model characteristics:\n",
    "\n",
    "```\n",
    "‚úÖ Good: \"GPT-4 Turbo excels at complex reasoning, code generation, \n",
    "         and detailed analysis. Best for tasks requiring deep understanding.\"\n",
    "\n",
    "‚ùå Bad: \"GPT-4 is a model.\"\n",
    "```\n",
    "\n",
    "Include:\n",
    "- Main strengths\n",
    "- Typical use cases\n",
    "- Special capabilities\n",
    "- Known limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ LLM candidates JSON format\n",
    "- ‚úÖ Adding new models manually\n",
    "- ‚úÖ Generating model embeddings\n",
    "- ‚úÖ Creating custom configurations\n",
    "- ‚úÖ API integration\n",
    "- ‚úÖ Best practices for model selection\n",
    "\n",
    "### Key Files Created:\n",
    "1. `data/example_data/llm_candidates/my_custom_llm.json` - New LLM pool\n",
    "2. `my_custom_config.yaml` - Configuration using new models\n",
    "\n",
    "### Next Steps:\n",
    "- **[10_Creating_Custom_Datasets.ipynb](10_Creating_Custom_Datasets.ipynb)** - Create training data\n",
    "- **[03_Training_Single_Round_Routers.ipynb](03_Training_Single_Round_Routers.ipynb)** - Train with new models\n",
    "- **[11_Advanced_Customization.ipynb](11_Advanced_Customization.ipynb)** - Advanced techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
